{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0   # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        # Device configuration\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Q-Network and Target Network\n",
    "        self.model = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_model = DQN(state_size, action_size).to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        self.update_target_model()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # Copy weights from model to target_model\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            act_values = self.model(state)\n",
    "        return torch.argmax(act_values).item()\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor([i[0] for i in minibatch]).squeeze(1).to(self.device)\n",
    "        actions = torch.LongTensor([i[1] for i in minibatch]).to(self.device)\n",
    "        rewards = torch.FloatTensor([i[2] for i in minibatch]).to(self.device)\n",
    "        next_states = torch.FloatTensor([i[3] for i in minibatch]).squeeze(1).to(self.device)\n",
    "        dones = torch.FloatTensor([i[4] for i in minibatch]).to(self.device)\n",
    "        \n",
    "        # Current Q values\n",
    "        curr_Q = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Next Q values\n",
    "        next_Q = self.target_model(next_states).detach().max(1)[0]\n",
    "        expected_Q = rewards + (1 - dones) * self.gamma * next_Q\n",
    "        \n",
    "        # Loss\n",
    "        loss = F.mse_loss(curr_Q, expected_Q)\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_state_dict(torch.load(name))\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.model.state_dict(), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubantu/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/1000, Score: -196.41958151978207, Epsilon: 0.65\n",
      "Episode: 2/1000, Score: -94.84500848363716, Epsilon: 0.46\n",
      "Episode: 3/1000, Score: -425.4095772474037, Epsilon: 0.28\n",
      "Episode: 4/1000, Score: -113.19537266806978, Epsilon: 0.17\n",
      "Episode: 5/1000, Score: -40.61460428956168, Epsilon: 0.10\n",
      "Episode: 11/1000, Score: -71.18480961336127, Epsilon: 0.01\n",
      "Episode: 12/1000, Score: -294.8290987989826, Epsilon: 0.01\n",
      "Episode: 13/1000, Score: -526.5690013225253, Epsilon: 0.01\n",
      "Episode: 14/1000, Score: -475.2884701300843, Epsilon: 0.01\n",
      "Episode: 15/1000, Score: -724.5386846760891, Epsilon: 0.01\n",
      "Episode: 16/1000, Score: -768.2349961647341, Epsilon: 0.01\n",
      "Episode: 17/1000, Score: -912.7576070231828, Epsilon: 0.01\n",
      "Episode: 18/1000, Score: -826.4290572314734, Epsilon: 0.01\n",
      "Episode: 19/1000, Score: -762.4153305859329, Epsilon: 0.01\n",
      "Episode: 20/1000, Score: -444.6288607795797, Epsilon: 0.01\n",
      "Episode: 21/1000, Score: -287.0508917341099, Epsilon: 0.01\n",
      "Episode: 24/1000, Score: -442.93274933694835, Epsilon: 0.01\n",
      "Episode: 25/1000, Score: -238.93666914683303, Epsilon: 0.01\n",
      "Episode: 27/1000, Score: -29.450094678037033, Epsilon: 0.01\n",
      "Episode: 28/1000, Score: -216.37316618969493, Epsilon: 0.01\n",
      "Episode: 30/1000, Score: -124.34402682436422, Epsilon: 0.01\n",
      "Episode: 31/1000, Score: -151.8352103662982, Epsilon: 0.01\n",
      "Episode: 32/1000, Score: -670.047434427139, Epsilon: 0.01\n",
      "Episode: 33/1000, Score: -172.75708229242318, Epsilon: 0.01\n",
      "Episode: 34/1000, Score: -17.762334348035026, Epsilon: 0.01\n",
      "Episode: 35/1000, Score: -394.4857556992682, Epsilon: 0.01\n",
      "Episode: 36/1000, Score: -201.3134506347618, Epsilon: 0.01\n",
      "Episode: 37/1000, Score: -209.45978019099687, Epsilon: 0.01\n",
      "Episode: 38/1000, Score: -39.81699988560091, Epsilon: 0.01\n",
      "Episode: 39/1000, Score: -238.34737670243462, Epsilon: 0.01\n",
      "Episode: 40/1000, Score: -84.34046889663007, Epsilon: 0.01\n",
      "Episode: 41/1000, Score: -311.93929294147625, Epsilon: 0.01\n",
      "Episode: 42/1000, Score: -171.30348323634783, Epsilon: 0.01\n",
      "Episode: 43/1000, Score: 2.1353741835852276, Epsilon: 0.01\n",
      "Episode: 44/1000, Score: -768.2118799708123, Epsilon: 0.01\n",
      "Episode: 45/1000, Score: -238.54149775169975, Epsilon: 0.01\n",
      "Episode: 46/1000, Score: 6.174652815827429, Epsilon: 0.01\n",
      "Episode: 47/1000, Score: -269.8949745543765, Epsilon: 0.01\n",
      "Episode: 48/1000, Score: -178.41042859471804, Epsilon: 0.01\n",
      "Episode: 49/1000, Score: -405.50242562854265, Epsilon: 0.01\n",
      "Episode: 50/1000, Score: -549.7145171735573, Epsilon: 0.01\n",
      "Episode: 51/1000, Score: -443.6404709053412, Epsilon: 0.01\n",
      "Episode: 52/1000, Score: -40.018906566136465, Epsilon: 0.01\n",
      "Episode: 53/1000, Score: -258.8670822761924, Epsilon: 0.01\n",
      "Episode: 54/1000, Score: -246.19974521623354, Epsilon: 0.01\n",
      "Episode: 55/1000, Score: -145.20752101755318, Epsilon: 0.01\n",
      "Episode: 56/1000, Score: 7.283005549019833, Epsilon: 0.01\n",
      "Episode: 57/1000, Score: -57.194055984604994, Epsilon: 0.01\n",
      "Episode: 58/1000, Score: -32.99159819498318, Epsilon: 0.01\n",
      "Episode: 59/1000, Score: -12.1086234895742, Epsilon: 0.01\n",
      "Episode: 60/1000, Score: -78.44443068341536, Epsilon: 0.01\n",
      "Episode: 61/1000, Score: -127.20122988473386, Epsilon: 0.01\n",
      "Episode: 62/1000, Score: -94.67507245811879, Epsilon: 0.01\n",
      "Episode: 63/1000, Score: -243.61746927545275, Epsilon: 0.01\n",
      "Episode: 64/1000, Score: -37.59853987389844, Epsilon: 0.01\n",
      "Episode: 65/1000, Score: -150.69501050260868, Epsilon: 0.01\n",
      "Episode: 66/1000, Score: -108.17659094656331, Epsilon: 0.01\n",
      "Episode: 67/1000, Score: -20.468715914163695, Epsilon: 0.01\n",
      "Episode: 68/1000, Score: -5.76358156248773, Epsilon: 0.01\n",
      "Episode: 69/1000, Score: -6.145609644110323, Epsilon: 0.01\n",
      "Episode: 70/1000, Score: -50.167955863787114, Epsilon: 0.01\n",
      "Episode: 72/1000, Score: -182.96774505269894, Epsilon: 0.01\n",
      "Episode: 73/1000, Score: -229.82311048357965, Epsilon: 0.01\n",
      "Episode: 75/1000, Score: -183.9802214137751, Epsilon: 0.01\n",
      "Episode: 76/1000, Score: -15.544567856697938, Epsilon: 0.01\n",
      "Episode: 79/1000, Score: -56.79876019433857, Epsilon: 0.01\n",
      "Episode: 80/1000, Score: -161.16036483992897, Epsilon: 0.01\n",
      "Episode: 82/1000, Score: -216.0592017140054, Epsilon: 0.01\n",
      "Episode: 83/1000, Score: -28.037601195168705, Epsilon: 0.01\n",
      "Episode: 87/1000, Score: 186.7391596833308, Epsilon: 0.01\n",
      "Episode: 88/1000, Score: -30.148024494568674, Epsilon: 0.01\n",
      "Episode: 91/1000, Score: -189.50016039976595, Epsilon: 0.01\n",
      "Episode: 94/1000, Score: -220.92769658442523, Epsilon: 0.01\n",
      "Episode: 96/1000, Score: -26.567687288733424, Epsilon: 0.01\n",
      "Episode: 98/1000, Score: 299.3784831479021, Epsilon: 0.01\n",
      "Episode: 99/1000, Score: 195.09248485523526, Epsilon: 0.01\n",
      "Episode: 103/1000, Score: -185.76523706784815, Epsilon: 0.01\n",
      "Episode: 106/1000, Score: -38.88231350946971, Epsilon: 0.01\n",
      "Episode: 112/1000, Score: -10.253736309820184, Epsilon: 0.01\n",
      "Episode: 115/1000, Score: 24.98524729368826, Epsilon: 0.01\n",
      "Episode: 118/1000, Score: 13.547287042745495, Epsilon: 0.01\n",
      "Episode: 119/1000, Score: 17.42923621361635, Epsilon: 0.01\n",
      "Episode: 120/1000, Score: 23.634470461871288, Epsilon: 0.01\n",
      "Episode: 121/1000, Score: 4.233926450873184, Epsilon: 0.01\n",
      "Episode: 124/1000, Score: 41.92935132210516, Epsilon: 0.01\n",
      "Episode: 132/1000, Score: -61.62448266112264, Epsilon: 0.01\n",
      "Episode: 137/1000, Score: 70.25568711661458, Epsilon: 0.01\n",
      "Episode: 142/1000, Score: -14.759081383065052, Epsilon: 0.01\n",
      "Episode: 156/1000, Score: 26.33384222039345, Epsilon: 0.01\n",
      "Episode: 162/1000, Score: -106.97315802903168, Epsilon: 0.01\n",
      "Episode: 169/1000, Score: -74.25270857261145, Epsilon: 0.01\n",
      "Episode: 173/1000, Score: -103.30248041658801, Epsilon: 0.01\n",
      "Episode: 181/1000, Score: -112.08080685587697, Epsilon: 0.01\n",
      "Episode: 183/1000, Score: -153.95334027772788, Epsilon: 0.01\n",
      "Episode: 185/1000, Score: -138.76771757309427, Epsilon: 0.01\n",
      "Episode: 188/1000, Score: -88.2519015266426, Epsilon: 0.01\n",
      "Episode: 191/1000, Score: -79.47368206584989, Epsilon: 0.01\n",
      "Episode: 195/1000, Score: -95.03652055969357, Epsilon: 0.01\n",
      "Episode: 196/1000, Score: -106.03293656059552, Epsilon: 0.01\n",
      "Episode: 197/1000, Score: -136.4801264374401, Epsilon: 0.01\n",
      "Episode: 198/1000, Score: -106.5309975213176, Epsilon: 0.01\n",
      "Episode: 199/1000, Score: -92.8450070914841, Epsilon: 0.01\n",
      "Episode: 201/1000, Score: -95.1734872518646, Epsilon: 0.01\n",
      "Episode: 202/1000, Score: -131.78155127334966, Epsilon: 0.01\n",
      "Episode: 203/1000, Score: -84.88968763671873, Epsilon: 0.01\n",
      "Episode: 205/1000, Score: -156.5222478497244, Epsilon: 0.01\n",
      "Episode: 206/1000, Score: -70.52503391656269, Epsilon: 0.01\n",
      "Episode: 207/1000, Score: -101.23309998329954, Epsilon: 0.01\n",
      "Episode: 208/1000, Score: -92.42916817261303, Epsilon: 0.01\n",
      "Episode: 209/1000, Score: -105.98503599182773, Epsilon: 0.01\n",
      "Episode: 211/1000, Score: -128.8834260682972, Epsilon: 0.01\n",
      "Episode: 216/1000, Score: -77.63914525259858, Epsilon: 0.01\n",
      "Episode: 219/1000, Score: -71.12192135947701, Epsilon: 0.01\n",
      "Episode: 224/1000, Score: -118.28333013329579, Epsilon: 0.01\n",
      "Episode: 238/1000, Score: -52.32840522482337, Epsilon: 0.01\n",
      "Episode: 242/1000, Score: -62.79659780320078, Epsilon: 0.01\n",
      "Episode: 243/1000, Score: -97.67434428400013, Epsilon: 0.01\n",
      "Episode: 244/1000, Score: -105.76168901889386, Epsilon: 0.01\n",
      "Episode: 245/1000, Score: -151.25875142051527, Epsilon: 0.01\n",
      "Episode: 246/1000, Score: -124.67462647273686, Epsilon: 0.01\n",
      "Episode: 247/1000, Score: -105.26167175984571, Epsilon: 0.01\n",
      "Episode: 248/1000, Score: -80.66025472305176, Epsilon: 0.01\n",
      "Episode: 250/1000, Score: -107.85777531318027, Epsilon: 0.01\n",
      "Episode: 251/1000, Score: -114.36331842303743, Epsilon: 0.01\n",
      "Episode: 253/1000, Score: -135.7060513594699, Epsilon: 0.01\n",
      "Episode: 254/1000, Score: -112.72743462587195, Epsilon: 0.01\n",
      "Episode: 255/1000, Score: -151.7965102234084, Epsilon: 0.01\n",
      "Episode: 256/1000, Score: -172.38974654941765, Epsilon: 0.01\n",
      "Episode: 257/1000, Score: -121.94997849207779, Epsilon: 0.01\n",
      "Episode: 259/1000, Score: -126.89196979483167, Epsilon: 0.01\n",
      "Episode: 261/1000, Score: -114.92822214163768, Epsilon: 0.01\n",
      "Episode: 266/1000, Score: -107.7305052978988, Epsilon: 0.01\n",
      "Episode: 267/1000, Score: -118.48908552523702, Epsilon: 0.01\n",
      "Episode: 269/1000, Score: -111.04033730334237, Epsilon: 0.01\n",
      "Episode: 270/1000, Score: -75.68148554972215, Epsilon: 0.01\n",
      "Episode: 272/1000, Score: -112.54923571889402, Epsilon: 0.01\n",
      "Episode: 273/1000, Score: -133.87404165609252, Epsilon: 0.01\n",
      "Episode: 274/1000, Score: -99.03514871140122, Epsilon: 0.01\n",
      "Episode: 275/1000, Score: -134.9830863861469, Epsilon: 0.01\n",
      "Episode: 284/1000, Score: -147.13750742168628, Epsilon: 0.01\n",
      "Episode: 285/1000, Score: -182.1317596792902, Epsilon: 0.01\n",
      "Episode: 286/1000, Score: -141.17183022058862, Epsilon: 0.01\n",
      "Episode: 287/1000, Score: -118.5325001862367, Epsilon: 0.01\n",
      "Episode: 293/1000, Score: -135.78751561736073, Epsilon: 0.01\n",
      "Episode: 301/1000, Score: -101.78126121821863, Epsilon: 0.01\n",
      "Forced QUIT!\n",
      "Forced QUIT!\n",
      "Forced QUIT!\n",
      "Episode: 304/1000, Score: -127.96277542953973, Epsilon: 0.01\n",
      "Forced QUIT!\n",
      "Forced QUIT!\n",
      "Forced QUIT!\n",
      "Forced QUIT!\n",
      "Forced QUIT!\n",
      "Forced QUIT!\n",
      "Forced QUIT!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 45\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Train the agent with batch of experiences\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[0;32m---> 45\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Update target model every 10 episodes\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[3], line 59\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(curr_Q, expected_Q)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Optimize the model\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/decorators.py:50\u001b[0m, in \u001b[0;36mdisable\u001b[0;34m(fn, recursive)\u001b[0m\n\u001b[1;32m     48\u001b[0m         fn \u001b[38;5;241m=\u001b[39m innermost_fn(fn)\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[0;32m---> 50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisableContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisableContext()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:406\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:826\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(filename\u001b[38;5;241m.\u001b[39mendswith(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    824\u001b[0m              importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mEXTENSION_SUFFIXES):\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[1;32m    828\u001b[0m \u001b[38;5;66;03m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # Create the CartPole environment\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Number of episodes to run\n",
    "    n_episodes = 1000\n",
    "    \n",
    "    # For plotting metrics\n",
    "    all_rewards = []\n",
    "    \n",
    "    for e in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        total_reward = 0\n",
    "        \n",
    "        for time in range(500):  # Max time steps in an episode\n",
    "            # Uncomment to render the environment (slows training)\n",
    "            # env.render()\n",
    "            \n",
    "            # Choose action\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            \n",
    "            # Remember experience\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update state and reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            # End episode if done\n",
    "            if done:\n",
    "                print(f\"Episode: {e+1}/{n_episodes}, Score: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
    "                break\n",
    "                \n",
    "            # Train the agent with batch of experiences\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "        \n",
    "        # Update target model every 10 episodes\n",
    "        if e % 10 == 0:\n",
    "            agent.update_target_model()\n",
    "            \n",
    "        all_rewards.append(total_reward)\n",
    "        \n",
    "        # Consider the environment solved if average reward over 100 episodes is 195+\n",
    "        if len(all_rewards) > 100 and np.mean(all_rewards[-100:]) > 30:\n",
    "            print(f\"Environment solved in {e+1} episodes!\")\n",
    "            agent.save(\"dqn_land.pt\")\n",
    "            break\n",
    "        \n",
    "        if len(all_rewards) > 300:\n",
    "            agent.save(\"dqn_land.pt\")\n",
    "            print(f\"Forced QUIT!\")\n",
    "            break \n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
